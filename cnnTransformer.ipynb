{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, csv_path, audio_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        audio_path = os.path.join(self.audio_dir, row['name'])\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        # Labels\n",
    "        category = row['cate']\n",
    "        distance = row['dist']\n",
    "        direction = row['dire']\n",
    "\n",
    "        # Convert labels to indices\n",
    "        cate_idx = {'AR': 0, 'Sniper': 1, 'nogun': 2}[category]\n",
    "        dist_idx = {'none': 0, '0m': 1, '50m': 2, '100m': 3, '200m': 4, '400m': 5, '600m': 6}[distance]\n",
    "        dire_idx = {'none': 0, 'center': 1, 'back': 2, 'front': 3, 'left': 4, 'right': 5}[direction]\n",
    "\n",
    "        return waveform, (cate_idx, dist_idx, dire_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, sample_rate):\n",
    "        self.transforms = torchaudio.transforms.Spectrogram(n_fft=512, hop_length=256)\n",
    "\n",
    "    def __call__(self, waveform):\n",
    "        return self.transforms(waveform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=1, hidden_dim=128, n_heads=4, num_layers=2, num_classes=(3, 7, 6)):\n",
    "        super(CNNTransformer, self).__init__()\n",
    "        \n",
    "        # CNN Feature Extractor\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc_proj = nn.Linear(2752, hidden_dim)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=n_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Fully Connected Layers for each task\n",
    "        self.fc_cate = nn.Linear(hidden_dim, num_classes[0])\n",
    "        self.fc_dist = nn.Linear(hidden_dim, num_classes[1])\n",
    "        self.fc_dire = nn.Linear(hidden_dim, num_classes[2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN\n",
    "        x = self.cnn(x)\n",
    "        \n",
    "        # Reshape CNN output for Transformer\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()  # B, H, W, C\n",
    "        x = x.view(x.size(0), x.size(1), -1)  # B, H, (W*C)\n",
    "        x = self.fc_proj(x)  # Project to hidden_dim\n",
    "        x = x.permute(1, 0, 2)  # seq_len, batch_size, hidden_dim\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)  # Aggregate over sequence length\n",
    "        \n",
    "        # Task-specific outputs\n",
    "        cate_out = self.fc_cate(x)\n",
    "        dist_out = self.fc_dist(x)\n",
    "        dire_out = self.fc_dire(x)\n",
    "        \n",
    "        return cate_out, dist_out, dire_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        cate_labels, dist_labels, dire_labels = [label.to(device) for label in labels]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        cate_out, dist_out, dire_out = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_cate = criterion(cate_out, cate_labels)\n",
    "        loss_dist = criterion(dist_out, dist_labels)\n",
    "        loss_dire = criterion(dire_out, dire_labels)\n",
    "        loss = loss_cate + loss_dist + loss_dire\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct_cate, correct_dist, correct_dire = 0, 0, 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            cate_labels, dist_labels, dire_labels = [label.to(device) for label in labels]\n",
    "\n",
    "            cate_out, dist_out, dire_out = model(inputs)\n",
    "\n",
    "            _, cate_pred = torch.max(cate_out, 1)\n",
    "            _, dist_pred = torch.max(dist_out, 1)\n",
    "            _, dire_pred = torch.max(dire_out, 1)\n",
    "\n",
    "            correct_cate += (cate_pred == cate_labels).sum().item()\n",
    "            correct_dist += (dist_pred == dist_labels).sum().item()\n",
    "            correct_dire += (dire_pred == dire_labels).sum().item()\n",
    "\n",
    "            total += cate_labels.size(0)\n",
    "\n",
    "    cate_acc = correct_cate / total\n",
    "    dist_acc = correct_dist / total\n",
    "    dire_acc = correct_dire / total\n",
    "\n",
    "    return cate_acc, dist_acc, dire_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Paths\n",
    "    csv_path = 'dataset1.csv'\n",
    "    audio_dir = 'gun_sound_v8'\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size = 16\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 20\n",
    "    sample_rate = 44100\n",
    "\n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    feature_extractor = FeatureExtractor(sample_rate)\n",
    "    dataset = AudioDataset(csv_path, audio_dir, transform=feature_extractor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Model, Loss, Optimizer\n",
    "    model = CNNTransformer().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model(model, dataloader, criterion, optimizer, device)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Evaluate Model\n",
    "    cate_acc, dist_acc, dire_acc = evaluate_model(model, dataloader, device)\n",
    "    print(f\"Test Accuracy - Category: {cate_acc:.4f}, Distance: {dist_acc:.4f}, Direction: {dire_acc:.4f}\")\n",
    "\n",
    "    # Save Model\n",
    "    torch.save(model.state_dict(), 'cnn_transformer_model.pth')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n",
      "  warnings.warn(\n",
      "c:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after CNN: torch.Size([16, 64, 32, 21])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (512x1344 and 262144x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 194\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy - Direction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 194\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 165\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    161\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    163\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 165\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m    168\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2], line 101\u001b[0m, in \u001b[0;36mCNNTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     99\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()  \u001b[38;5;66;03m# B, H, W, C\u001b[39;00m\n\u001b[0;32m    100\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# B, H, (W*C)\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Project to hidden_dim\u001b[39;00m\n\u001b[0;32m    102\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# seq_len, batch_size, hidden_dim\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Transformer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512x1344 and 262144x128)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Dataset Class with Mel-Spectrogram\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, csv_path, audio_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        audio_path = os.path.join(self.audio_dir, row['name'])\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        # Labels\n",
    "        category = row['cate']\n",
    "        direction = row['dire']\n",
    "\n",
    "        # Convert labels to indices\n",
    "        cate_idx = {'AR': 0, 'Sniper': 1, 'nogun': 2}[category]\n",
    "        dire_idx = {'none': 0, 'center': 1, 'back': 2, 'front': 3, 'left': 4, 'right': 5}[direction]\n",
    "\n",
    "        # Ensure direction is none if category is Noise\n",
    "        if cate_idx == 2:  # Noise\n",
    "            dire_idx = 0\n",
    "\n",
    "        return waveform, dire_idx\n",
    "\n",
    "# Feature Extraction with Mel-Spectrogram\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, sample_rate, n_mels=128):\n",
    "        self.transforms = nn.Sequential(\n",
    "            torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=1024, hop_length=512, n_mels=n_mels),\n",
    "            torchaudio.transforms.AmplitudeToDB()\n",
    "        )\n",
    "\n",
    "    def __call__(self, waveform):\n",
    "        return self.transforms(waveform)\n",
    "\n",
    "# Updated CNN-Transformer Model\n",
    "class CNNTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=1, hidden_dim=128, n_heads=4, num_layers=2, num_classes=6, n_mels=128):\n",
    "        super(CNNTransformer, self).__init__()\n",
    "        \n",
    "        # CNN Feature Extractor\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        # Projection Layer to match Transformer input\n",
    "        cnn_output_dim = self._get_cnn_output_dim(n_mels)\n",
    "        self.fc_proj = nn.Linear(cnn_output_dim, hidden_dim)\n",
    "\n",
    "            \n",
    "        # Transformer Encoder\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=n_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Fully Connected Layer for direction prediction\n",
    "        self.fc_dire = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def _get_cnn_output_dim(self, n_mels, n_frames=512):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, n_mels, n_frames)  # Simulate input (batch=1, channels=1, n_mels, time_steps)\n",
    "            output = self.cnn(dummy_input)\n",
    "            _, channels, height, width = output.size()\n",
    "            return channels * height * width  # Compute flattened size\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN\n",
    "        x = self.cnn(x)\n",
    "        print(f\"Shape after CNN: {x.shape}\")\n",
    "        \n",
    "        # Reshape CNN output for Transformer\n",
    "        batch_size = x.size(0)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()  # B, H, W, C\n",
    "        x = x.view(batch_size, x.size(1), -1)  # B, H, (W*C)\n",
    "        x = self.fc_proj(x)  # Project to hidden_dim\n",
    "        x = x.permute(1, 0, 2)  # seq_len, batch_size, hidden_dim\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)  # Aggregate over sequence length\n",
    "        \n",
    "        # Direction output\n",
    "        dire_out = self.fc_dire(x)\n",
    "        \n",
    "        return dire_out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# Main Script\n",
    "def main():\n",
    "    # Paths\n",
    "    csv_path = 'dataset1.csv'\n",
    "    audio_dir = 'gun_sound_v8'\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size = 16\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 20\n",
    "    sample_rate = 44100\n",
    "    n_mels = 128\n",
    "\n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    feature_extractor = FeatureExtractor(sample_rate, n_mels=n_mels)\n",
    "    dataset = AudioDataset(csv_path, audio_dir, transform=feature_extractor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    # Model, Loss, Optimizer\n",
    "    model = CNNTransformer(input_channels=1, hidden_dim=128, n_heads=4, num_layers=2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    # Evaluate Model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        print(f\"Test Accuracy - Direction: {correct / total:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
